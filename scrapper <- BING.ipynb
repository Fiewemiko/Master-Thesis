{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "[INFO] START: site=money.pl, rok=2021\n",
      "======================================\n",
      "[INFO] Wygenerowano 4 zapytaÅ„ dla roku 2021.\n",
      "\n",
      "[INFO] Zapytanie: site:money.pl prognoza inflacji 2021\n",
      "[INFO]  Pobieranie wynikÃ³w start=1\n",
      "[INFO]  Pobieranie wynikÃ³w start=11\n",
      "[INFO]  Pobieranie wynikÃ³w start=21\n",
      "[INFO]  Pobieranie wynikÃ³w start=31\n",
      "[INFO]  Pobieranie wynikÃ³w start=41\n",
      "[INFO]  Pobieranie wynikÃ³w start=51\n",
      "[INFO]  Pobieranie wynikÃ³w start=61\n",
      "[INFO]  Pobieranie wynikÃ³w start=71\n",
      "[INFO]  Pobieranie wynikÃ³w start=81\n",
      "[INFO]  Pobieranie wynikÃ³w start=91\n",
      "\n",
      "[INFO] Zapytanie: site:money.pl prognoza PKB 2021\n",
      "[INFO]  Pobieranie wynikÃ³w start=1\n",
      "[INFO]  Pobieranie wynikÃ³w start=11\n",
      "[INFO]  Pobieranie wynikÃ³w start=21\n",
      "[INFO]  Pobieranie wynikÃ³w start=31\n",
      "[INFO]  Pobieranie wynikÃ³w start=41\n",
      "[INFO]  Pobieranie wynikÃ³w start=51\n",
      "[INFO]  Pobieranie wynikÃ³w start=61\n",
      "[INFO]  Pobieranie wynikÃ³w start=71\n",
      "[INFO]  Pobieranie wynikÃ³w start=81\n",
      "[INFO]  Pobieranie wynikÃ³w start=91\n",
      "\n",
      "[INFO] Zapytanie: site:money.pl prognoza wzrostu gospodarczego 2021\n",
      "[INFO]  Pobieranie wynikÃ³w start=1\n",
      "[INFO]  Pobieranie wynikÃ³w start=11\n",
      "[INFO]  Pobieranie wynikÃ³w start=21\n",
      "[INFO]  Pobieranie wynikÃ³w start=31\n",
      "[INFO]  Pobieranie wynikÃ³w start=41\n",
      "[INFO]  Pobieranie wynikÃ³w start=51\n",
      "[INFO]  Pobieranie wynikÃ³w start=61\n",
      "[INFO]  Pobieranie wynikÃ³w start=71\n",
      "[INFO]  Pobieranie wynikÃ³w start=81\n",
      "[INFO]  Pobieranie wynikÃ³w start=91\n",
      "\n",
      "[INFO] Zapytanie: site:money.pl projekcja inflacji 2021\n",
      "[INFO]  Pobieranie wynikÃ³w start=1\n",
      "[INFO]  Pobieranie wynikÃ³w start=11\n",
      "[INFO]  Pobieranie wynikÃ³w start=21\n",
      "[INFO]  Pobieranie wynikÃ³w start=31\n",
      "[INFO]  Pobieranie wynikÃ³w start=41\n",
      "[INFO]  Pobieranie wynikÃ³w start=51\n",
      "[INFO]  Pobieranie wynikÃ³w start=61\n",
      "[INFO]  Pobieranie wynikÃ³w start=71\n",
      "[INFO]  Pobieranie wynikÃ³w start=81\n",
      "[INFO]  Pobieranie wynikÃ³w start=91\n",
      "[INFO] Zebrano 278 rekordÃ³w dla site=money.pl\n",
      "[INFO] Zapisano 278 rekordÃ³w â†’ money_pl_2021.csv\n",
      "\n",
      "======================================\n",
      "[INFO] START: site=money.pl, rok=2022\n",
      "======================================\n",
      "[INFO] Wygenerowano 4 zapytaÅ„ dla roku 2022.\n",
      "\n",
      "[INFO] Zapytanie: site:money.pl prognoza inflacji 2022\n",
      "[INFO]  Pobieranie wynikÃ³w start=1\n",
      "[INFO]  Pobieranie wynikÃ³w start=11\n",
      "[INFO]  Pobieranie wynikÃ³w start=21\n",
      "[INFO]  Pobieranie wynikÃ³w start=31\n",
      "[INFO]  Pobieranie wynikÃ³w start=41\n",
      "[INFO]  Pobieranie wynikÃ³w start=51\n",
      "[INFO]  Pobieranie wynikÃ³w start=61\n",
      "[INFO]  Pobieranie wynikÃ³w start=71\n",
      "[INFO]  Pobieranie wynikÃ³w start=81\n",
      "[INFO]  Pobieranie wynikÃ³w start=91\n",
      "\n",
      "[INFO] Zapytanie: site:money.pl prognoza PKB 2022\n",
      "[INFO]  Pobieranie wynikÃ³w start=1\n",
      "[INFO]  Pobieranie wynikÃ³w start=11\n",
      "[INFO]  Pobieranie wynikÃ³w start=21\n",
      "[INFO]  Pobieranie wynikÃ³w start=31\n",
      "[INFO]  Pobieranie wynikÃ³w start=41\n",
      "[INFO]  Pobieranie wynikÃ³w start=51\n",
      "[INFO]  Pobieranie wynikÃ³w start=61\n",
      "[INFO]  Pobieranie wynikÃ³w start=71\n",
      "[INFO]  Pobieranie wynikÃ³w start=81\n",
      "[INFO]  Pobieranie wynikÃ³w start=91\n",
      "\n",
      "[INFO] Zapytanie: site:money.pl prognoza wzrostu gospodarczego 2022\n",
      "[INFO]  Pobieranie wynikÃ³w start=1\n",
      "[INFO]  Pobieranie wynikÃ³w start=11\n",
      "[INFO]  Pobieranie wynikÃ³w start=21\n",
      "[INFO]  Pobieranie wynikÃ³w start=31\n",
      "[INFO]  Pobieranie wynikÃ³w start=41\n",
      "[INFO]  Pobieranie wynikÃ³w start=51\n",
      "[INFO]  Pobieranie wynikÃ³w start=61\n",
      "[INFO]  Pobieranie wynikÃ³w start=71\n",
      "[INFO]  Pobieranie wynikÃ³w start=81\n",
      "[INFO]  Pobieranie wynikÃ³w start=91\n",
      "\n",
      "[INFO] Zapytanie: site:money.pl projekcja inflacji 2022\n",
      "[INFO]  Pobieranie wynikÃ³w start=1\n",
      "[INFO]  Pobieranie wynikÃ³w start=11\n",
      "[INFO]  Pobieranie wynikÃ³w start=21\n",
      "[INFO]  Pobieranie wynikÃ³w start=31\n",
      "[INFO]  Pobieranie wynikÃ³w start=41\n",
      "[INFO]  Pobieranie wynikÃ³w start=51\n",
      "[INFO]  Pobieranie wynikÃ³w start=61\n",
      "[INFO]  Pobieranie wynikÃ³w start=71\n",
      "[INFO]  Pobieranie wynikÃ³w start=81\n",
      "[INFO]  Pobieranie wynikÃ³w start=91\n",
      "[INFO] Zebrano 275 rekordÃ³w dla site=money.pl\n",
      "[INFO] Zapisano 275 rekordÃ³w â†’ money_pl_2022.csv\n",
      "\n",
      "======================================\n",
      "[INFO] START: site=money.pl, rok=2023\n",
      "======================================\n",
      "[INFO] Wygenerowano 4 zapytaÅ„ dla roku 2023.\n",
      "\n",
      "[INFO] Zapytanie: site:money.pl prognoza inflacji 2023\n",
      "[INFO]  Pobieranie wynikÃ³w start=1\n",
      "[INFO]  Pobieranie wynikÃ³w start=11\n",
      "[INFO]  Pobieranie wynikÃ³w start=21\n",
      "[INFO]  Pobieranie wynikÃ³w start=31\n",
      "[INFO]  Pobieranie wynikÃ³w start=41\n",
      "[INFO]  Pobieranie wynikÃ³w start=51\n",
      "[INFO]  Pobieranie wynikÃ³w start=61\n",
      "[INFO]  Pobieranie wynikÃ³w start=71\n",
      "[INFO]  Pobieranie wynikÃ³w start=81\n",
      "[INFO]  Pobieranie wynikÃ³w start=91\n",
      "\n",
      "[INFO] Zapytanie: site:money.pl prognoza PKB 2023\n",
      "[INFO]  Pobieranie wynikÃ³w start=1\n",
      "[WARN]  Brak danych / bÅ‚Ä…d z Google.\n",
      "\n",
      "[INFO] Zapytanie: site:money.pl prognoza wzrostu gospodarczego 2023\n",
      "[INFO]  Pobieranie wynikÃ³w start=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] Google Search (site:money.pl prognoza PKB 2023) start=1: 429 Client Error: Too Many Requests for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyAira1haMCscJl7q46XUOQ2cjEGCKPy7uE&cx=a4187dfcf85e64282&q=site%3Amoney.pl+prognoza+PKB+2023&start=1&num=10&hl=pl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  Pobieranie wynikÃ³w start=11\n",
      "[INFO]  Pobieranie wynikÃ³w start=21\n",
      "[INFO]  Pobieranie wynikÃ³w start=31\n",
      "[WARN]  Brak danych / bÅ‚Ä…d z Google.\n",
      "\n",
      "[INFO] Zapytanie: site:money.pl projekcja inflacji 2023\n",
      "[INFO]  Pobieranie wynikÃ³w start=1\n",
      "[WARN]  Brak danych / bÅ‚Ä…d z Google.\n",
      "[INFO] Zebrano 121 rekordÃ³w dla site=money.pl\n",
      "[INFO] Zapisano 121 rekordÃ³w â†’ money_pl_2023.csv\n",
      "\n",
      "[INFO] Zrobione dla wszystkich serwisÃ³w i lat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] Google Search (site:money.pl prognoza wzrostu gospodarczego 2023) start=31: 429 Client Error: Too Many Requests for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyAira1haMCscJl7q46XUOQ2cjEGCKPy7uE&cx=a4187dfcf85e64282&q=site%3Amoney.pl+prognoza+wzrostu+gospodarczego+2023&start=31&num=10&hl=pl\n",
      "[ERROR] Google Search (site:money.pl projekcja inflacji 2023) start=1: 429 Client Error: Too Many Requests for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyAira1haMCscJl7q46XUOQ2cjEGCKPy7uE&cx=a4187dfcf85e64282&q=site%3Amoney.pl+projekcja+inflacji+2023&start=1&num=10&hl=pl\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Skrypt:\n",
    "- UÅ¼ywa Google Custom Search JSON API do wyszukiwania w domenie bankier.pl.\n",
    "- Generuje zapytania o prognozy makro (inflacja, PKB, wzrost gospodarczy) dla lat 2021â€“2023.\n",
    "- Pobiera podstawowe dane z wynikÃ³w i zapisuje je do CSV.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "GOOGLE_API_ENDPOINT = \"https://www.googleapis.com/customsearch/v1\"\n",
    "\n",
    "# >>> TU USTAWIASZ SWOJE DANE <<<\n",
    "# MoÅ¼esz:\n",
    "#  - ALBO ustawiÄ‡ zmienne Å›rodowiskowe GOOGLE_API_KEY / GOOGLE_CX_ID,\n",
    "#  - ALBO wkleiÄ‡ je tutaj wprost (na wÅ‚asnÄ… odpowiedzialnoÅ›Ä‡).\n",
    "GOOGLE_API_KEY_DEFAULT = \"AIzaSyAira1haMCscJl7q46XUOQ2cjEGCKPy7uE\"\n",
    "GOOGLE_CX_ID_DEFAULT = \"a4187dfcf85e64282\"\n",
    "\n",
    "\n",
    "# ðŸ“° SERWISY, KTÃ“RE CHCEMY PRZELECIEÄ†\n",
    "ALLOWED_SITES = [\n",
    "    \"money.pl\",  # zmieniono z pulsbiznesu.pl na money.pl\n",
    "]\n",
    "\n",
    "# ðŸ“… LATA\n",
    "\n",
    "\n",
    "# âŒ FRAGMENTY URL-I, KTÃ“RE OZNACZAJÄ„ PAYWALL / PREMIUM\n",
    "BLOCKED_SUBSTRINGS = [\n",
    "    \"/premium\",\n",
    "    \"/paywall\",\n",
    "    \"token=\",\n",
    "]\n",
    "\n",
    "\n",
    "def _is_allowed_url(url: str) -> bool:\n",
    "    \"\"\"Sprawdza, czy URL nie jest pÅ‚atny / dziwny.\"\"\"\n",
    "    if not url:\n",
    "        return False\n",
    "    lower = url.lower()\n",
    "    for bad in BLOCKED_SUBSTRINGS:\n",
    "        if bad in lower:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "YEARS = [2021, 2022, 2023]\n",
    "def build_queries_for_year(year: int) -> list[str]:\n",
    "    base_phrases = [\n",
    "        \"prognoza inflacji\",\n",
    "        \"prognoza PKB\",\n",
    "        \"prognoza wzrostu gospodarczego\",\n",
    "        \"prognoza bezrobocia\",\n",
    "    ]\n",
    "    return [f\"{phrase} {year}\" for phrase in base_phrases]\n",
    "\n",
    "\n",
    "def google_search(query: str, api_key: str, cx_id: str,\n",
    "                  start: int = 1, timeout: float = 20.0) -> dict:\n",
    "    params = {\n",
    "        \"key\": api_key,\n",
    "        \"cx\": cx_id,\n",
    "        \"q\": query,\n",
    "        \"start\": start,\n",
    "        \"num\": 10,\n",
    "        \"hl\": \"pl\",\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(GOOGLE_API_ENDPOINT, params=params, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Google Search ({query}) start={start}: {e}\", file=sys.stderr)\n",
    "        return {}\n",
    "\n",
    "\n",
    "def _extract_domain(url: str) -> str:\n",
    "    try:\n",
    "        return urlparse(url).netloc\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _extract_date_from_metatags(pagemap: dict) -> str | None:\n",
    "    if not isinstance(pagemap, dict):\n",
    "        return None\n",
    "    metatags = pagemap.get(\"metatags\", [])\n",
    "    if not isinstance(metatags, list):\n",
    "        return None\n",
    "\n",
    "    date_keys = [\n",
    "        \"article:published_time\",\n",
    "        \"og:updated_time\",\n",
    "        \"publish-date\",\n",
    "        \"pubdate\",\n",
    "        \"datePublished\",\n",
    "        \"date\",\n",
    "    ]\n",
    "\n",
    "    for mt in metatags:\n",
    "        if not isinstance(mt, dict):\n",
    "            continue\n",
    "        for key in date_keys:\n",
    "            if key in mt and mt[key]:\n",
    "                return mt[key]\n",
    "    return None\n",
    "\n",
    "\n",
    "def search_site_for_queries(\n",
    "    site: str,\n",
    "    queries: list[str],\n",
    "    api_key: str,\n",
    "    cx_id: str,\n",
    "    max_pages_per_query: int = 10,\n",
    "    delay: float = 1.0,\n",
    ") -> list[dict]:\n",
    "    \"\"\"Jedzie po wszystkich zapytaniach dla danego serwisu i zwraca listÄ™ rekordÃ³w.\"\"\"\n",
    "    records = []\n",
    "    seen_urls = set()\n",
    "\n",
    "    for q in queries:\n",
    "        full_q = f\"site:{site} {q}\"\n",
    "        print(f\"\\n[INFO] Zapytanie: {full_q}\")\n",
    "\n",
    "        for page_index in range(max_pages_per_query):\n",
    "            start = 1 + page_index * 10  # 1, 11, 21, ...\n",
    "            print(f\"[INFO]  Pobieranie wynikÃ³w start={start}\")\n",
    "\n",
    "            data = google_search(full_q, api_key=api_key, cx_id=cx_id, start=start)\n",
    "            if not data:\n",
    "                print(\"[WARN]  Brak danych / bÅ‚Ä…d z Google.\")\n",
    "                break\n",
    "\n",
    "            items = data.get(\"items\", [])\n",
    "            if not items:\n",
    "                print(\"[INFO]  Brak dalszych wynikÃ³w.\")\n",
    "                break\n",
    "\n",
    "            for item in items:\n",
    "                url = item.get(\"link\")\n",
    "                if not url:\n",
    "                    continue\n",
    "                if not _is_allowed_url(url):\n",
    "                    continue\n",
    "                if url in seen_urls:\n",
    "                    continue\n",
    "\n",
    "                seen_urls.add(url)\n",
    "\n",
    "                title_search = item.get(\"title\", \"\") or \"\"\n",
    "                snippet_search = item.get(\"snippet\", \"\") or \"\"\n",
    "                domain = _extract_domain(url)\n",
    "                pagemap = item.get(\"pagemap\", {}) or {}\n",
    "                google_detected_date = _extract_date_from_metatags(pagemap) or \"\"\n",
    "\n",
    "                rec = {\n",
    "                    \"query\": full_q,\n",
    "                    \"url\": url,\n",
    "                    \"domain\": domain,\n",
    "                    \"title_search\": title_search,\n",
    "                    \"snippet_search\": snippet_search,\n",
    "                    \"google_detected_date\": google_detected_date,\n",
    "                }\n",
    "                records.append(rec)\n",
    "\n",
    "            time.sleep(delay)\n",
    "\n",
    "    print(f\"[INFO] Zebrano {len(records)} rekordÃ³w dla site={site}\")\n",
    "    return records\n",
    "\n",
    "\n",
    "def save_to_csv(records: list[dict], path: str) -> None:\n",
    "    if not records:\n",
    "        print(f\"[INFO] Brak rekordÃ³w do zapisania â†’ {path}\")\n",
    "        return\n",
    "\n",
    "    fieldnames = [\n",
    "        \"query\",\n",
    "        \"url\",\n",
    "        \"domain\",\n",
    "        \"title_search\",\n",
    "        \"snippet_search\",\n",
    "        \"google_detected_date\",\n",
    "    ]\n",
    "\n",
    "    file_exists = os.path.exists(path)\n",
    "\n",
    "    with open(path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(records)\n",
    "\n",
    "    print(f\"[INFO] Zapisano {len(records)} rekordÃ³w â†’ {path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\", GOOGLE_API_KEY_DEFAULT)\n",
    "    cx_id = os.getenv(\"GOOGLE_CX_ID\", GOOGLE_CX_ID_DEFAULT)\n",
    "\n",
    "    if \"TUTAJ_WKLEJ\" in api_key or \"TUTAJ_WKLEJ\" in cx_id:\n",
    "        print(\"[ERROR] UzupeÅ‚nij GOOGLE_API_KEY_DEFAULT / GOOGLE_CX_ID_DEFAULT albo ustaw ENV.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    for site in ALLOWED_SITES:\n",
    "        for year in YEARS:\n",
    "            print(\"\\n======================================\")\n",
    "            print(f\"[INFO] START: site={site}, rok={year}\")\n",
    "            print(\"======================================\")\n",
    "\n",
    "            queries = build_queries_for_year(year)\n",
    "            print(f\"[INFO] Wygenerowano {len(queries)} zapytaÅ„ dla roku {year}.\")\n",
    "\n",
    "            records = search_site_for_queries(\n",
    "                site,\n",
    "                queries,\n",
    "                api_key=api_key,\n",
    "                cx_id=cx_id,\n",
    "                max_pages_per_query=10,   # zmieniono z powrotem na 10\n",
    "                delay=1.0,\n",
    "            )\n",
    "\n",
    "            filename_prefix = site.replace(\".\", \"_\")\n",
    "            out_path = f\"{filename_prefix}_{year}.csv\"\n",
    "            save_to_csv(records, out_path)\n",
    "\n",
    "    print(\"\\n[INFO] Zrobione dla wszystkich serwisÃ³w i lat.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Znaleziono 5 plikÃ³w: ['bankier_google_results_2021_2023_2023_2.csv', 'bankier_google_results_2021_2023_clean.csv', 'bankier_google_results_2021_2023_2.csv', 'bankier_google_results_2021_2023_2023.csv', 'bankier_google_results_2021_2023.csv']\n",
      "PoÅ‚Ä…czono 1017 rekordÃ³w (przed czyszczeniem)\n",
      "Po usuniÄ™ciu duplikatÃ³w: 473 rekordÃ³w\n",
      "âœ… Zapisano scalony i oczyszczony plik: bankier_results_full_clean.csv\n",
      "ðŸ—‘ï¸ UsuniÄ™to bankier_google_results_2021_2023_2023_2.csv\n",
      "ðŸ—‘ï¸ UsuniÄ™to bankier_google_results_2021_2023_clean.csv\n",
      "ðŸ—‘ï¸ UsuniÄ™to bankier_google_results_2021_2023_2.csv\n",
      "ðŸ—‘ï¸ UsuniÄ™to bankier_google_results_2021_2023_2023.csv\n",
      "ðŸ—‘ï¸ UsuniÄ™to bankier_google_results_2021_2023.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 1ï¸âƒ£ Wyszukaj wszystkie CSV, ktÃ³re chcesz poÅ‚Ä…czyÄ‡\n",
    "csv_files = glob.glob(\"bankier_google_results_2021_2023*.csv\")\n",
    "\n",
    "print(f\"Znaleziono {len(csv_files)} plikÃ³w:\", csv_files)\n",
    "\n",
    "# 2ï¸âƒ£ Wczytaj i sklej wszystkie wiersze w jeden DataFrame\n",
    "df_all = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "\n",
    "print(f\"PoÅ‚Ä…czono {len(df_all)} rekordÃ³w (przed czyszczeniem)\")\n",
    "\n",
    "# 3ï¸âƒ£ UsuÅ„ duplikaty po URL\n",
    "df_all = df_all.drop_duplicates(subset=[\"url\"])\n",
    "\n",
    "print(f\"Po usuniÄ™ciu duplikatÃ³w: {len(df_all)} rekordÃ³w\")\n",
    "\n",
    "# 4ï¸âƒ£ Zapisz do jednego, ostatecznego pliku\n",
    "final_path = \"bankier_results_full_clean.csv\"\n",
    "df_all.to_csv(final_path, index=False)\n",
    "\n",
    "print(f\"âœ… Zapisano scalony i oczyszczony plik: {final_path}\")\n",
    "\n",
    "# 5ï¸âƒ£ (Opcjonalnie) usuÅ„ oryginalne pliki po scaleniu\n",
    "for f in csv_files:\n",
    "    os.remove(f)\n",
    "    print(f\"ðŸ—‘ï¸ UsuniÄ™to {f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Dodatkowy skrypt:\n",
    "Dodaje tylko zapytania o 'prognoza bezrobocia' do istniejÄ…cych plikÃ³w CSV.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "\n",
    "GOOGLE_API_ENDPOINT = \"https://www.googleapis.com/customsearch/v1\"\n",
    "GOOGLE_API_KEY_DEFAULT = \"AIzaSyAira1haMCscJl7q46XUOQ2cjEGCKPy7uE\"\n",
    "GOOGLE_CX_ID_DEFAULT = \"a4187dfcf85e64282\"\n",
    "\n",
    "ALLOWED_SITES = [\"money.pl\"]\n",
    "YEARS = [2021, 2022, 2023]\n",
    "\n",
    "BLOCKED_SUBSTRINGS = [\"/premium\", \"/paywall\", \"token=\"]\n",
    "\n",
    "def _is_allowed_url(url: str) -> bool:\n",
    "    if not url:\n",
    "        return False\n",
    "    lower = url.lower()\n",
    "    for bad in BLOCKED_SUBSTRINGS:\n",
    "        if bad in lower:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def google_search(query: str, api_key: str, cx_id: str,\n",
    "                  start: int = 1, timeout: float = 20.0) -> dict:\n",
    "    params = {\n",
    "        \"key\": api_key,\n",
    "        \"cx\": cx_id,\n",
    "        \"q\": query,\n",
    "        \"start\": start,\n",
    "        \"num\": 10,\n",
    "        \"hl\": \"pl\",\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(GOOGLE_API_ENDPOINT, params=params, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Google Search ({query}) start={start}: {e}\", file=sys.stderr)\n",
    "        return {}\n",
    "\n",
    "def _extract_domain(url: str) -> str:\n",
    "    try:\n",
    "        return urlparse(url).netloc\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _extract_date_from_metatags(pagemap: dict) -> str | None:\n",
    "    if not isinstance(pagemap, dict):\n",
    "        return None\n",
    "    metatags = pagemap.get(\"metatags\", [])\n",
    "    if not isinstance(metatags, list):\n",
    "        return None\n",
    "    date_keys = [\n",
    "        \"article:published_time\",\n",
    "        \"og:updated_time\",\n",
    "        \"publish-date\",\n",
    "        \"pubdate\",\n",
    "        \"datePublished\",\n",
    "        \"date\",\n",
    "    ]\n",
    "    for mt in metatags:\n",
    "        if not isinstance(mt, dict):\n",
    "            continue\n",
    "        for key in date_keys:\n",
    "            if key in mt and mt[key]:\n",
    "                return mt[key]\n",
    "    return None\n",
    "\n",
    "def search_unemployment_only(\n",
    "    site: str,\n",
    "    year: int,\n",
    "    api_key: str,\n",
    "    cx_id: str,\n",
    "    max_pages: int = 10,\n",
    "    delay: float = 1.0,\n",
    ") -> list[dict]:\n",
    "    \"\"\"Szuka tylko 'prognoza bezrobocia {year}' dla danego serwisu.\"\"\"\n",
    "    records = []\n",
    "    seen_urls = set()\n",
    "    \n",
    "    query = f\"site:{site} prognoza bezrobocia {year}\"\n",
    "    print(f\"\\n[INFO] Zapytanie: {query}\")\n",
    "    \n",
    "    for page_index in range(max_pages):\n",
    "        start = 1 + page_index * 10\n",
    "        print(f\"[INFO]  Pobieranie wynikÃ³w start={start}\")\n",
    "        \n",
    "        data = google_search(query, api_key=api_key, cx_id=cx_id, start=start)\n",
    "        if not data:\n",
    "            print(\"[WARN]  Brak danych / bÅ‚Ä…d z Google.\")\n",
    "            break\n",
    "        \n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            print(\"[INFO]  Brak dalszych wynikÃ³w.\")\n",
    "            break\n",
    "        \n",
    "        for item in items:\n",
    "            url = item.get(\"link\")\n",
    "            if not url or not _is_allowed_url(url) or url in seen_urls:\n",
    "                continue\n",
    "            \n",
    "            seen_urls.add(url)\n",
    "            \n",
    "            rec = {\n",
    "                \"query\": query,\n",
    "                \"url\": url,\n",
    "                \"domain\": _extract_domain(url),\n",
    "                \"title_search\": item.get(\"title\", \"\") or \"\",\n",
    "                \"snippet_search\": item.get(\"snippet\", \"\") or \"\",\n",
    "                \"google_detected_date\": _extract_date_from_metatags(item.get(\"pagemap\", {})) or \"\",\n",
    "            }\n",
    "            records.append(rec)\n",
    "        \n",
    "        time.sleep(delay)\n",
    "    \n",
    "    print(f\"[INFO] Zebrano {len(records)} nowych rekordÃ³w (bezrobocie)\")\n",
    "    return records\n",
    "\n",
    "def append_to_csv(records: list[dict], path: str) -> None:\n",
    "    \"\"\"Dopisuje nowe rekordy do istniejÄ…cego CSV (lub tworzy nowy).\"\"\"\n",
    "    if not records:\n",
    "        print(f\"[INFO] Brak nowych rekordÃ³w do dodania â†’ {path}\")\n",
    "        return\n",
    "    \n",
    "    fieldnames = [\n",
    "        \"query\",\n",
    "        \"url\",\n",
    "        \"domain\",\n",
    "        \"title_search\",\n",
    "        \"snippet_search\",\n",
    "        \"google_detected_date\",\n",
    "    ]\n",
    "    \n",
    "    # Wczytaj istniejÄ…ce URL-e, Å¼eby uniknÄ…Ä‡ duplikatÃ³w\n",
    "    existing_urls = set()\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        existing_urls = set(df[\"url\"].tolist())\n",
    "    \n",
    "    # Filtruj tylko nowe URL-e\n",
    "    new_records = [r for r in records if r[\"url\"] not in existing_urls]\n",
    "    \n",
    "    if not new_records:\n",
    "        print(f\"[INFO] Wszystkie URL-e juÅ¼ istniejÄ… â†’ {path}\")\n",
    "        return\n",
    "    \n",
    "    with open(path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not existing_urls:  # Plik nie istniaÅ‚\n",
    "            writer.writeheader()\n",
    "        writer.writerows(new_records)\n",
    "    \n",
    "    print(f\"[INFO] Dodano {len(new_records)} nowych rekordÃ³w â†’ {path}\")\n",
    "\n",
    "def main_unemployment():\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\", GOOGLE_API_KEY_DEFAULT)\n",
    "    cx_id = os.getenv(\"GOOGLE_CX_ID\", GOOGLE_CX_ID_DEFAULT)\n",
    "    \n",
    "    for site in ALLOWED_SITES:\n",
    "        for year in YEARS:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"[INFO] Dodawanie 'bezrobocie' dla {site}, rok {year}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            records = search_unemployment_only(\n",
    "                site=site,\n",
    "                year=year,\n",
    "                api_key=api_key,\n",
    "                cx_id=cx_id,\n",
    "                max_pages=10,\n",
    "                delay=1.0,\n",
    "            )\n",
    "            \n",
    "            filename = f\"{site.replace('.', '_')}_{year}.csv\"\n",
    "            append_to_csv(records, filename)\n",
    "    \n",
    "    print(\"\\n[INFO] âœ… Dodano prognozy bezrobocia do wszystkich plikÃ³w.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_unemployment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
