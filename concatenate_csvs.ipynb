{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] üöÄ Rozpoczynam ≈ÇƒÖczenie plik√≥w CSV...\n",
      "[INFO] üìÇ Pliki ≈∫r√≥d≈Çowe: csv ze stron\n",
      "[INFO] üìÅ Pliki wynikowe zostanƒÖ zapisane w: csv ze stron\n",
      "\n",
      "[INFO] ≈ÅƒÖczenie plik√≥w dla forsal.pl...\n",
      "[WARN]  ‚ö† Plik csv ze stron/forsal_pl_2021.csv nie istnieje\n",
      "[WARN]  ‚ö† Plik csv ze stron/forsal_pl_2022.csv nie istnieje\n",
      "[WARN]  ‚ö† Plik csv ze stron/forsal_pl_2023.csv nie istnieje\n",
      "[WARN] Brak plik√≥w do po≈ÇƒÖczenia dla forsal.pl\n",
      "\n",
      "[INFO] ≈ÅƒÖczenie plik√≥w dla businessinsider.com.pl...\n",
      "[INFO]  ‚úì Wczytano csv ze stron/businessinsider_com_pl_2021.csv: 100 rekord√≥w\n",
      "[INFO]  ‚úì Wczytano csv ze stron/businessinsider_com_pl_2022.csv: 100 rekord√≥w\n",
      "[INFO]  ‚úì Wczytano csv ze stron/businessinsider_com_pl_2023.csv: 100 rekord√≥w\n",
      "[INFO]  Usuniƒôto 118 duplikat√≥w\n",
      "[INFO]  ‚úì Zapisano csv ze stron/full_businessinsider_com_pl.csv: 182 rekord√≥w\n",
      "\n",
      "[INFO] ≈ÅƒÖczenie plik√≥w dla pap.pl...\n",
      "[INFO]  ‚úì Wczytano csv ze stron/pap_pl_2021.csv: 82 rekord√≥w\n",
      "[INFO]  ‚úì Wczytano csv ze stron/pap_pl_2022.csv: 96 rekord√≥w\n",
      "[INFO]  ‚úì Wczytano csv ze stron/pap_pl_2023.csv: 100 rekord√≥w\n",
      "[INFO]  Usuniƒôto 129 duplikat√≥w\n",
      "[INFO]  ‚úì Zapisano csv ze stron/full_pap_pl.csv: 149 rekord√≥w\n",
      "\n",
      "[INFO] ≈ÅƒÖczenie plik√≥w dla tvn24.pl...\n",
      "[INFO]  ‚úì Wczytano csv ze stron/tvn24_pl_2021.csv: 100 rekord√≥w\n",
      "[INFO]  ‚úì Wczytano csv ze stron/tvn24_pl_2022.csv: 100 rekord√≥w\n",
      "[INFO]  ‚úì Wczytano csv ze stron/tvn24_pl_2023.csv: 100 rekord√≥w\n",
      "[INFO]  Usuniƒôto 127 duplikat√≥w\n",
      "[INFO]  ‚úì Zapisano csv ze stron/full_tvn24_pl.csv: 173 rekord√≥w\n",
      "\n",
      "[INFO] ≈ÅƒÖczenie plik√≥w dla obserwatorfinansowy.pl...\n",
      "[INFO]  ‚úì Wczytano csv ze stron/obserwatorfinansowy_pl_2021.csv: 18 rekord√≥w\n",
      "[INFO]  ‚úì Wczytano csv ze stron/obserwatorfinansowy_pl_2022.csv: 24 rekord√≥w\n",
      "[INFO]  ‚úì Wczytano csv ze stron/obserwatorfinansowy_pl_2023.csv: 32 rekord√≥w\n",
      "[INFO]  Usuniƒôto 32 duplikat√≥w\n",
      "[INFO]  ‚úì Zapisano csv ze stron/full_obserwatorfinansowy_pl.csv: 42 rekord√≥w\n",
      "\n",
      "[INFO] ≈ÅƒÖczenie plik√≥w dla pie.net.pl...\n",
      "[WARN]  ‚ö† Plik csv ze stron/pie_net_pl_2021.csv nie istnieje\n",
      "[WARN]  ‚ö† Plik csv ze stron/pie_net_pl_2022.csv nie istnieje\n",
      "[WARN]  ‚ö† Plik csv ze stron/pie_net_pl_2023.csv nie istnieje\n",
      "[WARN] Brak plik√≥w do po≈ÇƒÖczenia dla pie.net.pl\n",
      "\n",
      "[INFO] ‚úÖ Zako≈Ñczono ≈ÇƒÖczenie plik√≥w!\n",
      "[INFO] üìä Utworzono 4 plik√≥w full_*.csv\n",
      "[INFO] üìù ≈ÅƒÖcznie 546 unikalnych artyku≈Ç√≥w\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Skrypt do ≈ÇƒÖczenia plik√≥w CSV dla ka≈ºdej strony ze wszystkich lat\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Lista stron\n",
    "ALLOWED_SITES = [\n",
    "    \"forsal.pl\",\n",
    "    \"businessinsider.com.pl\",\n",
    "    \"pap.pl\",\n",
    "    \"tvn24.pl\",\n",
    "    \"obserwatorfinansowy.pl\",\n",
    "    \"pie.net.pl\"\n",
    "]\n",
    "\n",
    "# Lata\n",
    "YEARS = [2021, 2022, 2023]\n",
    "\n",
    "# Folder ≈∫r√≥d≈Çowy i wyj≈õciowy\n",
    "SOURCE_FOLDER = \"csv ze stron\"\n",
    "OUTPUT_FOLDER = \"csv ze stron\"\n",
    "\n",
    "def concatenate_site_csvs(site: str, years: list, source_folder: str = \".\", output_folder: str = \".\"):\n",
    "    \"\"\"\n",
    "    ≈ÅƒÖczy pliki CSV dla danej strony ze wszystkich lat.\n",
    "    \n",
    "    Args:\n",
    "        site: nazwa strony (np. 'forsal.pl')\n",
    "        years: lista lat do po≈ÇƒÖczenia\n",
    "        source_folder: folder gdzie znajdujƒÖ siƒô pliki CSV\n",
    "        output_folder: folder gdzie zapisaƒá wynikowy plik\n",
    "    \"\"\"\n",
    "    site_normalized = site.replace('.', '_')\n",
    "    dataframes = []\n",
    "    \n",
    "    print(f\"\\n[INFO] ≈ÅƒÖczenie plik√≥w dla {site}...\")\n",
    "    \n",
    "    for year in years:\n",
    "        filename = os.path.join(source_folder, f\"{site_normalized}_{year}.csv\")\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            try:\n",
    "                df = pd.read_csv(filename)\n",
    "                # Dodaj kolumnƒô z rokiem dla ≈Çatwiejszej analizy\n",
    "                df['year'] = year\n",
    "                dataframes.append(df)\n",
    "                print(f\"[INFO]  ‚úì Wczytano {filename}: {len(df)} rekord√≥w\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR]  ‚úó B≈ÇƒÖd przy wczytywaniu {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"[WARN]  ‚ö† Plik {filename} nie istnieje\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        print(f\"[WARN] Brak plik√≥w do po≈ÇƒÖczenia dla {site}\")\n",
    "        return None\n",
    "    \n",
    "    # Po≈ÇƒÖcz wszystkie dataframe'y\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Usu≈Ñ duplikaty na podstawie URL\n",
    "    initial_count = len(combined_df)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['url'], keep='first')\n",
    "    duplicates_removed = initial_count - len(combined_df)\n",
    "    \n",
    "    if duplicates_removed > 0:\n",
    "        print(f\"[INFO]  Usuniƒôto {duplicates_removed} duplikat√≥w\")\n",
    "    \n",
    "    # Sortuj wed≈Çug daty (je≈õli istnieje) lub roku\n",
    "    if 'google_detected_date' in combined_df.columns:\n",
    "        combined_df = combined_df.sort_values('google_detected_date')\n",
    "    else:\n",
    "        combined_df = combined_df.sort_values('year')\n",
    "    \n",
    "    # Zapisz po≈ÇƒÖczony plik jako full_{site}.csv\n",
    "    output_filename = os.path.join(output_folder, f\"full_{site_normalized}.csv\")\n",
    "    combined_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"[INFO]  ‚úì Zapisano {output_filename}: {len(combined_df)} rekord√≥w\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def main():\n",
    "    # Utw√≥rz folder wyj≈õciowy je≈õli nie istnieje\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "    \n",
    "    print(\"[INFO] üöÄ Rozpoczynam ≈ÇƒÖczenie plik√≥w CSV...\")\n",
    "    print(f\"[INFO] üìÇ Pliki ≈∫r√≥d≈Çowe: {SOURCE_FOLDER}\")\n",
    "    print(f\"[INFO] üìÅ Pliki wynikowe zostanƒÖ zapisane w: {OUTPUT_FOLDER}\")\n",
    "    \n",
    "    total_files = 0\n",
    "    total_records = 0\n",
    "    \n",
    "    # Po≈ÇƒÖcz pliki dla ka≈ºdej strony\n",
    "    for site in ALLOWED_SITES:\n",
    "        result = concatenate_site_csvs(site, YEARS, SOURCE_FOLDER, OUTPUT_FOLDER)\n",
    "        if result is not None:\n",
    "            total_files += 1\n",
    "            total_records += len(result)\n",
    "    \n",
    "    print(f\"\\n[INFO] ‚úÖ Zako≈Ñczono ≈ÇƒÖczenie plik√≥w!\")\n",
    "    print(f\"[INFO] üìä Utworzono {total_files} plik√≥w full_*.csv\")\n",
    "    print(f\"[INFO] üìù ≈ÅƒÖcznie {total_records} unikalnych artyku≈Ç√≥w\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "money_df = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
