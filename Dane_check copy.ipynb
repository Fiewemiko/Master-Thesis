{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dane makroekonomiczne – rozszerzona weryfikacja prognoz\n",
        "Usprawnienia obejmują walidację wejścia, lepszą agregację, nowe metryki oraz dodatkowe testy statystyczne i wizualizacje.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import json, re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import mstats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stały katalog bazowy i walidacja wejścia\n",
        "BASE_DIR = Path('wyniki LLM')\n",
        "FILES = {\n",
        "    'bankier': 'bankier_llm_extracted.csv',\n",
        "    'business insider': 'businessinsider_llm_extracted.csv',\n",
        "    'money': 'money_llm_extracted.csv',\n",
        "    'obserwator finansowy': 'obserwatorfinansowy_llm_extracted_clean.csv',\n",
        "    'pap': 'pap_llm_extracted.csv',\n",
        "    'tvn24': 'tvn24_llm_extracted.csv',\n",
        "}\n",
        "\n",
        "rows = []\n",
        "data_frames = []\n",
        "for name, fname in FILES.items():\n",
        "    path = BASE_DIR / fname\n",
        "    if not path.exists():\n",
        "        rows.append({'source': name, 'articles': np.nan, 'note': f'missing: {path}'})\n",
        "        continue\n",
        "    try:\n",
        "        df_src = pd.read_csv(path)\n",
        "    except Exception as e:\n",
        "        rows.append({'source': name, 'articles': np.nan, 'note': f'ERROR: {e}'})\n",
        "        continue\n",
        "    rows.append({'source': name, 'articles': len(df_src), 'note': 'ok'})\n",
        "    df_src['news_source'] = name\n",
        "    data_frames.append(df_src)\n",
        "\n",
        "articles_per_source = pd.DataFrame(rows)\n",
        "display(articles_per_source)\n",
        "\n",
        "sns.set_theme(style='whitegrid')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=articles_per_source, x='source', y='articles', palette='viridis', hue='note', dodge=False, legend=False)\n",
        "plt.title('Articles per Source (validated)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "if not data_frames:\n",
        "    raise SystemExit('Brak poprawnie wczytanych plików z BASE_DIR')\n",
        "\n",
        "df = pd.concat(data_frames, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Walidacja i normalizacja prognoz\n",
        "Liczba odrzuconych rekordów jest raportowana według przyczyny oraz liczba zachowanych prognoz na zmienną/rok.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse forecasts_json -> normalized long df (one row per variable/year)\n",
        "valid_vars = {'gdp','inflation','unemployment','deficit','public_debt','interest_rate','fx','wages','other'}\n",
        "parse_stats = Counter()\n",
        "parsed_rows = []\n",
        "\n",
        "def _parse_row(raw: str, stats_counter: Counter):\n",
        "    try:\n",
        "        items = json.loads(raw)\n",
        "    except Exception:\n",
        "        stats_counter['json_error'] += 1\n",
        "        return []\n",
        "    out = []\n",
        "    for f in items:\n",
        "        stats_counter['items_total'] += 1\n",
        "        if not isinstance(f, dict) or 'variable' not in f or 'value' not in f:\n",
        "            stats_counter['invalid_shape'] += 1\n",
        "            continue\n",
        "        var = str(f.get('variable', '')).strip().lower()\n",
        "        if var not in valid_vars:\n",
        "            stats_counter['unknown_variable'] += 1\n",
        "            continue\n",
        "        horizon = str(f.get('horizon', '')).strip()\n",
        "        m = re.search(r'\\b(\\d{4})\\b', horizon)\n",
        "        if not m:\n",
        "            stats_counter['missing_year'] += 1\n",
        "            continue\n",
        "        year = int(m.group(1))\n",
        "        val = pd.to_numeric(str(f.get('value', '')).replace(',', '.'), errors='coerce')\n",
        "        if pd.isna(val):\n",
        "            stats_counter['non_numeric'] += 1\n",
        "            continue\n",
        "        stats_counter[f'kept_{var}'] += 1\n",
        "        out.append({'variable': var, 'year': year, 'value': float(val)})\n",
        "    return out\n",
        "\n",
        "for _, r in df.iterrows():\n",
        "    parse_stats['rows_total'] += 1\n",
        "    recs = _parse_row(r.get('forecasts_json', '[]'), parse_stats)\n",
        "    parse_stats['rows_with_forecast'] += int(len(recs) > 0)\n",
        "    for rr in recs:\n",
        "        rr.update({\n",
        "            'url': r.get('url'),\n",
        "            'who': r.get('main_topic') or r.get('title_search') or '',\n",
        "            'news_source': r.get('news_source'),\n",
        "        })\n",
        "        parsed_rows.append(rr)\n",
        "\n",
        "forecasts_long = pd.DataFrame(parsed_rows)\n",
        "\n",
        "print('=== Statystyki walidacji ===')\n",
        "for k in sorted(parse_stats):\n",
        "    print(f\"{k}: {parse_stats[k]}\")\n",
        "\n",
        "missing_by_var_year = (forecasts_long.groupby(['variable', 'year']).size()\n",
        "                       .rename('n')\n",
        "                       .reset_index())\n",
        "print('\n",
        "Liczba prognoz na (zmienna, rok):')\n",
        "display(missing_by_var_year.head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agregacja prognoz i kontrola duplikatów\n",
        "Średnia, mediana i odchylenie standardowe dla wielu prognoz tego samego autora/roku.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key_vars = ['gdp', 'public_debt', 'deficit', 'inflation', 'unemployment', 'interest_rate']\n",
        "forecast_stats = (\n",
        "    forecasts_long[forecasts_long['variable'].isin(key_vars)]\n",
        "    .groupby(['year', 'who', 'news_source', 'variable'])\n",
        "    .agg(\n",
        "        forecast_mean=('value', 'mean'),\n",
        "        forecast_median=('value', 'median'),\n",
        "        forecast_std=('value', 'std'),\n",
        "        n=('value', 'count'),\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "wide_mean = forecast_stats.pivot(index=['year', 'who'], columns='variable', values='forecast_mean').reset_index()\n",
        "wide_median = forecast_stats.pivot(index=['year', 'who'], columns='variable', values='forecast_median').reset_index()\n",
        "\n",
        "print('Przykładowe wiersze (średnie prognozy):')\n",
        "display(wide_mean.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Łączenie z wartościami rzeczywistymi i raport braków\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('dane_makroekonomiczne.json', 'r', encoding='utf-8') as f:\n",
        "    actual_raw = json.load(f)\n",
        "\n",
        "name_map = {\n",
        "    'gdp': 'Realny wzrost PKB (% r/r)',\n",
        "    'inflation': 'Inflacja CPI (średnioroczna, %)',\n",
        "    'interest_rate': 'Stopa referencyjna NBP (średnioroczna, %)',\n",
        "    'unemployment': 'Stopa bezrobocia (BAEL / Eurostat, %)',\n",
        "    'deficit': 'Deficyt sektora GG (% PKB)',\n",
        "    'public_debt': 'Dług publiczny (GG, % PKB)',\n",
        "}\n",
        "\n",
        "ind_by_name = {ind['name']: ind for ind in actual_raw.get('indicators', [])}\n",
        "actual_rows = []\n",
        "for var, ind_name in name_map.items():\n",
        "    ind = ind_by_name.get(ind_name)\n",
        "    if not ind:\n",
        "        continue\n",
        "    for y, vals in ind.get('values', {}).items():\n",
        "        try:\n",
        "            year = int(y)\n",
        "        except Exception:\n",
        "            continue\n",
        "        val = vals.get('eurostat_gus')\n",
        "        if val is None:\n",
        "            val = vals.get('oryginalne')\n",
        "        if val is None:\n",
        "            continue\n",
        "        actual_rows.append({'variable': var, 'year': year, 'actual': float(val)})\n",
        "\n",
        "actual_df = pd.DataFrame(actual_rows)\n",
        "\n",
        "comparison = forecast_stats.merge(actual_df, on=['variable', 'year'], how='left')\n",
        "missing_actual_share = comparison['actual'].isna().mean()\n",
        "print(f'Brakujące wartości actual: {missing_actual_share:.1%} z {len(comparison)} wierszy')\n",
        "print('Rozkład braków per zmienna:')\n",
        "display(comparison.groupby('variable')['actual'].apply(lambda s: s.isna().mean()).to_frame('missing_share'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metryki jakości prognoz (MAE, MAPE, sMAPE, RMSE) z winsoryzacją APE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filtr obserwacji z rzeczywistą wartością\n",
        "comparison_valid = comparison[comparison['actual'].notna()].copy()\n",
        "\n",
        "# Obliczenia błędów\n",
        "comparison_valid['error'] = comparison_valid['forecast_mean'] - comparison_valid['actual']\n",
        "comparison_valid['abs_error'] = comparison_valid['error'].abs()\n",
        "comparison_valid['pct_error'] = np.where(\n",
        "    comparison_valid['actual'].abs() > 1e-9,\n",
        "    comparison_valid['error'] / comparison_valid['actual'] * 100,\n",
        "    np.nan,\n",
        ")\n",
        "comparison_valid['abs_pct_error'] = comparison_valid['pct_error'].abs()\n",
        "\n",
        "# Winsoryzacja skrajnych wartości procentowych\n",
        "comparison_valid['abs_pct_error_w'] = mstats.winsorize(comparison_valid['abs_pct_error'], limits=[0, 0.05])\n",
        "comparison_valid['smape'] = 200 * np.abs(comparison_valid['forecast_mean'] - comparison_valid['actual']) / (\n",
        "    np.abs(comparison_valid['forecast_mean']) + np.abs(comparison_valid['actual'])\n",
        ")\n",
        "\n",
        "rmse_by_var = comparison_valid.groupby('variable')['error'].apply(lambda s: np.sqrt(np.mean(np.square(s)))).rename('RMSE')\n",
        "\n",
        "summary_by_var = (\n",
        "    comparison_valid.groupby('variable').agg(\n",
        "        Mean_Error=('error', 'mean'),\n",
        "        Std_Error=('error', 'std'),\n",
        "        MAE=('abs_error', 'mean'),\n",
        "        Median_AE=('abs_error', 'median'),\n",
        "        MAPE_pct=('abs_pct_error', 'mean'),\n",
        "        MAPE_winsor_pct=('abs_pct_error_w', 'mean'),\n",
        "        sMAPE=('smape', 'mean'),\n",
        "        N=('error', 'count'),\n",
        "    )\n",
        ").join(rmse_by_var).round(3)\n",
        "print('\\n=== Błędy prognoz według zmiennych ===')\n",
        "display(summary_by_var)\n",
        "\n",
        "\n",
        "summary_by_year = (\n",
        "    comparison_valid.groupby('year').agg(\n",
        "        Mean_Error=('error', 'mean'),\n",
        "        Std_Error=('error', 'std'),\n",
        "        MAE=('abs_error', 'mean'),\n",
        "        MAPE_pct=('abs_pct_error', 'mean'),\n",
        "        sMAPE=('smape', 'mean'),\n",
        "        N=('error', 'count'),\n",
        "    ).round(3)\n",
        ")\n",
        "",
        "print('\\n=== Błędy prognoz według roku ===')\n",
        "display(summary_by_year)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testy statystyczne porównujące źródła/roczniki\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Porównanie błędów między źródłami (t-test lub Wilcoxon w wersji rank-sum)\n",
        "source_stats = comparison_valid.groupby('news_source')['error'].agg(['count', 'mean'])\n",
        "print('Błędy per źródło:')\n",
        "display(source_stats)\n",
        "\n",
        "sources = [s for s, n in source_stats['count'].items() if n >= 5]\n",
        "if len(sources) >= 2:\n",
        "    s1, s2 = sources[:2]\n",
        "    e1 = comparison_valid.loc[comparison_valid['news_source'] == s1, 'error']\n",
        "    e2 = comparison_valid.loc[comparison_valid['news_source'] == s2, 'error']\n",
        "    t_res = stats.ttest_ind(e1, e2, equal_var=False, nan_policy='omit')\n",
        "    u_res = stats.ranksums(e1, e2)\n",
        "    print(f\"\n",
        "T-test ({s1} vs {s2}): stat={t_res.statistic:.3f}, p={t_res.pvalue:.4f}\")\n",
        "    print(f\"Rank-sum ({s1} vs {s2}): stat={u_res.statistic:.3f}, p={u_res.pvalue:.4f}\")\n",
        "else:\n",
        "    print('Za mało danych na testy między źródłami (wymagane >=5 obserwacji na źródło).')\n",
        "\n",
        "# Trend błędu w czasie (regresja liniowa)\n",
        "lin_res = stats.linregress(comparison_valid['year'], comparison_valid['error'])\n",
        "print(f\"\n",
        "Trend błędu w czasie: slope={lin_res.slope:.4f}, p-value={lin_res.pvalue:.4f}, r^2={lin_res.rvalue**2:.3f}\")\n",
        "\n",
        "# Test KS porównujący rozkłady błędów dla dwóch największych źródeł\n",
        "source_sizes = comparison_valid['news_source'].value_counts()\n",
        "if len(source_sizes) >= 2:\n",
        "    s1, s2 = source_sizes.index[:2]\n",
        "    ks_res = stats.ks_2samp(\n",
        "        comparison_valid.loc[comparison_valid['news_source'] == s1, 'error'],\n",
        "        comparison_valid.loc[comparison_valid['news_source'] == s2, 'error'],\n",
        "        alternative='two-sided',\n",
        "    )\n",
        "    print(f\"KS-test ({s1} vs {s2}): stat={ks_res.statistic:.3f}, p={ks_res.pvalue:.4f}\")\n",
        "else:\n",
        "    print('Za mało źródeł na test KS.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wizualizacje: boxplot/violin, KDE i heatmapy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set_palette('tab10')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "sns.boxplot(data=comparison_valid, x='variable', y='error', ax=axes[0])\n",
        "axes[0].axhline(0, color='red', linestyle='--', linewidth=1)\n",
        "axes[0].set_title('Boxplot błędów per zmienna')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "sns.violinplot(data=comparison_valid, x='year', y='error', inner='quartile', ax=axes[1])\n",
        "axes[1].axhline(0, color='red', linestyle='--', linewidth=1)\n",
        "axes[1].set_title('Violin plot błędów per rok')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# KDE błędów\n",
        "plt.figure(figsize=(10, 6))\n",
        "for var, grp in comparison_valid.groupby('variable'):\n",
        "    sns.kdeplot(grp['error'], label=var, fill=False)\n",
        "plt.axvline(0, color='black', linestyle='--')\n",
        "plt.title('Gęstości błędów prognoz')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Heatmapa MAPE per (zmienna, rok)\n",
        "heatmap_data = comparison_valid.pivot_table(index='variable', columns='year', values='abs_pct_error', aggfunc='mean')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='mako', cbar_kws={'label': 'MAPE (%)'})\n",
        "plt.title('MAPE średnie wg zmiennej i roku')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Powyższe komórki dodają walidację wejścia, pełniejszą agregację, dodatkowe metryki (RMSE, sMAPE), testy statystyczne oraz nowe wizualizacje.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}