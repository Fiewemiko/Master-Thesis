{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai lxml beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "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\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "281f946f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 121)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<string>:121\u001b[0;36m\u001b[0m\n\u001b[0;31m    except Exception as e:\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Extractor dla BusinessInsider (businessinsider.com.pl) – analogiczny do wersji\n",
    "obserwatora, ale z własnymi selektorami HTML.\n",
    "\"\"\"\n",
    "import os, sys, time, json, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------ KONFIGURACJA ------------\n",
    "INPUT_CSV = \"full_businessinsider_com_pl.csv\"\n",
    "OUTPUT_CSV = \"businessinsider_llm_extracted.csv\"\n",
    "CHECKPOINT_CSV = \"checkpoint_progress_businessinsider.csv\"\n",
    "MAX_ARTICLES = 800\n",
    "MODEL_NAME = \"gpt-4.1-mini\"\n",
    "CHECKPOINT_INTERVAL = 10\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"pl-PL,pl;q=0.9,en-US;q=0.7,en;q=0.6\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "client = OpenAI(timeout=60.0)\n",
    "\n",
    "# ------------ CHECKPOINTING ------------\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_CSV):\n",
    "        try:\n",
    "            checkpoint_df = pd.read_csv(CHECKPOINT_CSV)\n",
    "            if len(checkpoint_df) > 0:\n",
    "                last_idx = checkpoint_df[\"processed_idx\"].max()\n",
    "                print(f\"[INFO] Wznawiam od artykułu {last_idx + 1}\")\n",
    "                return last_idx, checkpoint_df.to_dict(\"records\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Błąd przy wczytywaniu checkpoint: {e}\")\n",
    "    return -1, []\n",
    "\n",
    "def save_checkpoint(results, last_processed_idx):\n",
    "    try:\n",
    "        checkpoint_data = []\n",
    "        for i, result in enumerate(results):\n",
    "            r = result.copy()\n",
    "            r[\"processed_idx\"] = i\n",
    "            checkpoint_data.append(r)\n",
    "        pd.DataFrame(checkpoint_data).to_csv(CHECKPOINT_CSV, index=False)\n",
    "        print(f\"[INFO] Checkpoint zapisany po artykule {last_processed_idx}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Błąd przy zapisywaniu checkpoint: {e}\")\n",
    "\n",
    "# ------------ POBIERANIE HTML ------------\n",
    "def fetch_once(url: str, timeout: float = 20.0) -> str | None:\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        resp.encoding = resp.apparent_encoding\n",
    "        return resp.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"[ERROR] Błąd pobierania {url}: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "def fetch_html(url: str, timeout: float = 20.0, max_retries: int = 3) -> str | None:\n",
    "    html = None\n",
    "    for attempt in range(max_retries):\n",
    "        html = fetch_once(url, timeout=timeout)\n",
    "        if html is not None:\n",
    "            break\n",
    "        wait = 2 ** attempt\n",
    "        print(f\"[WARN] Retry {attempt+1}/{max_retries} za {wait}s dla {url}\")\n",
    "        time.sleep(wait)\n",
    "    return html\n",
    "\n",
    "# ------------ EKSTRAKCJA TREŚCI ------------\n",
    "def extract_article_text_businessinsider(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Próbuje złapać treść z układu BusinessInsider. Szuka kontenerów article /\n",
    "    article_content / grid, a w razie czego zbiera paragrafy.\n",
    "    \"\"\"\n",
    "    if not html or len(html) < 50:\n",
    "        return \"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\", \"noscript\", \"iframe\", \"header\", \"footer\", \"nav\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        # najpierw <article>\n",
    "        article = soup.find(\"article\")\n",
    "        candidates = []\n",
    "        if article:\n",
    "            candidates.append(article)\n",
    "\n",
    "        # typowe kontenery body\n",
    "        for cls in [\n",
    "            \"article_content\",\n",
    "            \"article__content\",\n",
    "            \"article-body\",\n",
    "            \"content-container\",\n",
    "            \"all-columns-wide\",\n",
    "            \"article\",\n",
    "        ]:\n",
    "            div = soup.find(\"div\", class_=lambda c: c and cls in c)\n",
    "            if div:\n",
    "                candidates.append(div)\n",
    "\n",
    "        text_parts = []\n",
    "        for node in candidates:\n",
    "            parts = [p.get_text(\" \", strip=True) for p in node.find_all(\"p\") if p.get_text(strip=True)]\n",
    "            if parts:\n",
    "                text_parts.extend(parts)\n",
    "        if not text_parts:\n",
    "            text_parts = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\") if p.get_text(strip=True)]\n",
    "\n",
    "text = \"\\n\".join(text_parts)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] extract_article_text_businessinsider error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ------------ WYWOŁANIE LLM ------------\n",
    "def call_llm_extract(article_text: str, url: str) -> dict | None:\n",
    "    system_prompt = (\n",
    "        \"Jesteś analitykiem ekonomicznym. \"\n",
    "        \"Dostajesz artykuł prasowy i masz z niego wyciągnąć wyłącznie konkretne prognozy makroekonomiczne \"\n",
    "        \"(inflacja, PKB, stopy procentowe, bezrobocie, wynagrodzenia, kurs walut, deficyt, dług publiczny). \"\n",
    "        \"Zwracaj szczególną uwagę na rozróżnienie między deficytem a długiem publicznym. \"\n",
    "        \"Jeżeli nie ma prognoz liczbowych — has_forecast=false.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "URL artykułu: {url}\n",
    "\n",
    "Pełny tekst artykułu:\n",
    "\"\"\"{article_text}\"\"\"\n",
    "\n",
    "Zwróć odpowiedź jako JSON w schemacie:\n",
    "{{\n",
    "  \"has_forecast\": bool,\n",
    "  \"main_topic\": \"...\",\n",
    "  \"country\": \"...\",\n",
    "  \"forecasts\": [\n",
    "    {{\n",
    "      \"variable\": \"...\",\n",
    "      \"value\": ...,\n",
    "      \"unit\": \"...\",\n",
    "      \"horizon\": \"...\",\n",
    "      \"direction\": \"...\",\n",
    "      \"who_forecasts\": \"...\",\n",
    "      \"quote\": \"...\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        start = time.time()\n",
    "        print(f\"[DEBUG] LLM start dla {url} (len_tekstu={len(article_text)})\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"[DEBUG] LLM done dla {url} w {elapsed:.1f}s\")\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Błąd wywołania OpenAI dla {url}: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "# ------------ GŁÓWNA PĘTLA ------------\n",
    "def main():\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"[ERROR] Brak zmiennej środowiskowej OPENAI_API_KEY.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Nie udało się wczytać {INPUT_CSV}: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if \"url\" not in df.columns:\n",
    "        print(\"[ERROR] Brak kolumny 'url' w CSV.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"[INFO] Wczytano {len(df)} rekordów.\")\n",
    "    df_subset = df.head(MAX_ARTICLES).copy()\n",
    "\n",
    "    last_processed_idx, results = load_checkpoint()\n",
    "    start_idx = last_processed_idx + 1\n",
    "\n",
    "    for idx, row in df_subset.iterrows():\n",
    "        if idx < start_idx:\n",
    "            continue\n",
    "        url = row[\"url\"]\n",
    "        print(f\"\n",
    "[INFO] ({idx+1}/{len(df_subset)}) Przetwarzam: {url}\")\n",
    "\n",
    "        html = fetch_html(url)\n",
    "        if not html:\n",
    "            print(\"[WARN] Brak HTML — pomijam.\")\n",
    "            continue\n",
    "\n",
    "        article_text = extract_article_text_businessinsider(html)\n",
    "        print(f\"[DEBUG] Długość tekstu: {len(article_text)}\")\n",
    "        if len(article_text) < 200:\n",
    "            print(f\"[WARN] Za mało tekstu ({len(article_text)}), pomijam.\")\n",
    "            continue\n",
    "\n",
    "        llm_data = call_llm_extract(article_text, url)\n",
    "        if llm_data is None:\n",
    "            print(\"[WARN] LLM zwrócił błąd lub timeout — pomijam.\")\n",
    "            continue\n",
    "\n",
    "        result_row = {\n",
    "            \"url\": url,\n",
    "            \"query\": row.get(\"query\", \"\"),\n",
    "            \"title_search\": row.get(\"title_search\", \"\"),\n",
    "            \"snippet_search\": row.get(\"snippet_search\", \"\"),\n",
    "            \"google_detected_date\": row.get(\"google_detected_date\", \"\"),\n",
    "            \"has_forecast\": llm_data.get(\"has_forecast\"),\n",
    "            \"main_topic\": llm_data.get(\"main_topic\"),\n",
    "            \"country\": llm_data.get(\"country\"),\n",
    "            \"forecasts_json\": json.dumps(llm_data.get(\"forecasts\", []), ensure_ascii=False),\n",
    "        }\n",
    "        results.append(result_row)\n",
    "\n",
    "        if len(results) % CHECKPOINT_INTERVAL == 0:\n",
    "            save_checkpoint(results, idx)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    if not results:\n",
    "        print(\"[WARN] Brak wyników — nic do zapisania.\")\n",
    "        return\n",
    "\n",
    "    pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\n",
    "[INFO] Zapisano {len(results)} rezultatów do {OUTPUT_CSV}\")\n",
    "\n",
    "    if os.path.exists(CHECKPOINT_CSV):\n",
    "        os.remove(CHECKPOINT_CSV)\n",
    "        print(\"[INFO] Usunięto checkpoint.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
