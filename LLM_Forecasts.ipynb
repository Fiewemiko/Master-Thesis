{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai lxml beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Wczytano 707 rekordów z /Users/filipniewczas/studia/cogni/magisterka kod/full_tvn24_pl.csv\n",
      "[INFO] Przetwarzam pierwsze 707 artykułów (testowo).\n",
      "\n",
      "[INFO] (1/707) Przetwarzam: https://tvn24.pl/biznes/z-kraju/inflacja-w-polsce-nowy-raport-nbp-projekcja-inflacji-i-wzrostu-pkb-na-rok-2021-2022-i-2023-st5481712\n",
      "[WARN] Zbyt mało tekstu (0 znaków), pomijam.\n",
      "\n",
      "[INFO] (2/707) Przetwarzam: https://tvn24.pl/biznes/z-kraju/pkb-i-inflacja-w-polsce-prognozy-na-2019-2021-najnowszy-raport-nbp-ra950959-ls4508942\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 297\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Usunięto checkpoint - przetwarzanie zakończone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[2], line 245\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m url \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[INFO] (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_subset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) Przetwarzam: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 245\u001b[0m html \u001b[38;5;241m=\u001b[39m fetch_html(url)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m html:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[WARN] Nie udało się pobrać HTML, pomijam.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 66\u001b[0m, in \u001b[0;36mfetch_html\u001b[0;34m(url, timeout, max_retries)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m         resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mHEADERS, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     67\u001b[0m         resp\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     68\u001b[0m         resp\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mapparent_encoding  \u001b[38;5;66;03m# Fix encoding issues\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     r\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_chunk_length()\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1208\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:1125\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1125\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------ KONFIGURACJA ------------\n",
    "\n",
    "INPUT_CSV = \"/Users/filipniewczas/studia/cogni/magisterka kod/full_tvn24_pl.csv\"   # Twój scalony plik z URL-ami\n",
    "OUTPUT_CSV = \"tvn24_llm_extracted.csv\"       # Tutaj zapiszą się wyniki\n",
    "CHECKPOINT_CSV = \"checkpoint_progress.csv\"      # Plik z checkpointami\n",
    "MAX_ARTICLES = 800                              # Na start: weź pierwsze 20 dla testów\n",
    "MODEL_NAME = \"gpt-5-nano\"                    # Podmień na model, którego używasz\n",
    "CHECKPOINT_INTERVAL = 20                        # Zapisuj co 20 artykułów\n",
    "\n",
    "client = OpenAI()  # używa zmiennej OPENAI_API_KEY z env\n",
    "\n",
    "# ------------ CHECKPOINTING ------------\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Wczytaj checkpoint - zwraca ostatni przetworzony indeks i dotychczasowe wyniki\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_CSV):\n",
    "        try:\n",
    "            checkpoint_df = pd.read_csv(CHECKPOINT_CSV)\n",
    "            if len(checkpoint_df) > 0:\n",
    "                last_idx = checkpoint_df['processed_idx'].max()\n",
    "                print(f\"[INFO] Wznawiam od artykułu {last_idx + 1}\")\n",
    "                return last_idx, checkpoint_df.to_dict('records')\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Błąd przy wczytywaniu checkpoint: {e}\")\n",
    "    \n",
    "    return -1, []\n",
    "\n",
    "def save_checkpoint(results, last_processed_idx):\n",
    "    \"\"\"Zapisz checkpoint z wynikami\"\"\"\n",
    "    try:\n",
    "        checkpoint_data = []\n",
    "        for i, result in enumerate(results):\n",
    "            result_with_idx = result.copy()\n",
    "            result_with_idx['processed_idx'] = i\n",
    "            checkpoint_data.append(result_with_idx)\n",
    "        \n",
    "        checkpoint_df = pd.DataFrame(checkpoint_data)\n",
    "        checkpoint_df.to_csv(CHECKPOINT_CSV, index=False)\n",
    "        print(f\"[INFO] Checkpoint zapisany po artykule {last_processed_idx}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Błąd przy zapisywaniu checkpoint: {e}\")\n",
    "\n",
    "# ------------ POBIERANIE I PARSOWANIE HTML ------------\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; FilipScraper/1.0; +mailto:twoj@mail.com)\"\n",
    "}\n",
    "\n",
    "def fetch_html(url: str, timeout: float = 20.0, max_retries: int = 3) -> str | None:\n",
    "    \"\"\"Pobiera HTML z retry mechanizmem.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            resp.encoding = resp.apparent_encoding  # Fix encoding issues\n",
    "            return resp.text\n",
    "        except requests.RequestException as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait = 2 ** attempt\n",
    "                print(f\"[WARN] Retry {attempt+1}/{max_retries} za {wait}s: {e}\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"[ERROR] Nie udało się pobrać {url}: {e}\", file=sys.stderr)\n",
    "    return None\n",
    "\n",
    "def extract_article_text_bankier(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Uniwersalny ekstraktor tekstu artykułu - obsługuje różne portale.\n",
    "    Obsługuje błędy parsowania z fallbackiem na lxml.\n",
    "    \"\"\"\n",
    "    if not html or len(html) < 100:\n",
    "        return \"\"\n",
    "    \n",
    "    # Próbuj kolejno różnych parserów\n",
    "    for parser in [\"lxml\", \"html.parser\", \"html5lib\"]:\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, parser)\n",
    "            \n",
    "            # Usuń niepotrzebne elementy\n",
    "            for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "                tag.decompose()\n",
    "            \n",
    "            # 1) Spróbuj złapać główny artykuł\n",
    "            article = soup.find(\"article\")\n",
    "            if article:\n",
    "                text = article.get_text(separator=\" \", strip=True)\n",
    "            else:\n",
    "                # 2) Próbuj różne typowe klasy i ID (Bankier, TVN24, inne)\n",
    "                main_div = (\n",
    "                    soup.find(\"div\", class_=\"articleBody\") or\n",
    "                    soup.find(\"div\", class_=\"article-content\") or\n",
    "                    soup.find(\"div\", id=\"articleContent\") or\n",
    "                    soup.find(\"div\", class_=\"article__body\") or  # TVN24\n",
    "                    soup.find(\"div\", class_=\"story-content\") or\n",
    "                    soup.find(\"div\", class_=\"entry-content\") or\n",
    "                    soup.find(\"main\") or\n",
    "                    soup.find(\"div\", attrs={\"itemprop\": \"articleBody\"})\n",
    "                )\n",
    "\n",
    "                if main_div:\n",
    "                    text = main_div.get_text(separator=\" \", strip=True)\n",
    "                else:\n",
    "                    # 3) Fallback – weź wszystkie paragrafy\n",
    "                    paragraphs = soup.find_all(\"p\")\n",
    "                    if paragraphs:\n",
    "                        text = \" \".join([p.get_text(strip=True) for p in paragraphs])\n",
    "                    else:\n",
    "                        # 4) Ostateczny fallback – cały tekst z body\n",
    "                        body = soup.body or soup\n",
    "                        text = body.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Przytnij bardzo długie artykuły i usuń nadmiar whitespace\n",
    "            text = \" \".join(text.split())\n",
    "            max_chars = 8000\n",
    "            return text[:max_chars]\n",
    "            \n",
    "        except Exception as e:\n",
    "            if parser == \"html5lib\":  # ostatnia próba\n",
    "                print(f\"[ERROR] Wszystkie parsery zawiodły: {e}\", file=sys.stderr)\n",
    "                return \"\"\n",
    "            continue  # spróbuj następnego parsera\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "# ------------ WYWOŁANIE API OPENAI ------------\n",
    "\n",
    "def call_llm_extract(article_text: str, url: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Wywołuje model językowy, żeby wyciągnąć prognozy makro.\n",
    "    Zwraca dict zgodny z ustalonym schematem lub None przy błędzie.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"Jesteś analitykiem ekonomicznym. \"\n",
    "        \"Dostajesz artykuł prasowy i masz z niego wyciągnąć wyłącznie konkretne prognozy makroekonomiczne \"\n",
    "        \"(inflacja, PKB, stopy procentowe, bezrobocie, wynagrodzenia, kurs walut, deficyt, dług publiczny). \"\n",
    "        \"Zwracaj szczególną uwagę na rozróżnienie między deficytem (saldo sektora finansów publicznych, zwykle wartości ujemne w % PKB) \"\n",
    "        \"a długiem publicznym (public debt, wartości dodatnie, np. 50% PKB). \"\n",
    "        \"Jeśli artykuł zawiera oba – oznacz je osobno: 'deficit' i 'public_debt'. \"\n",
    "        \"Nie powielaj prognoz o tej samej zmiennej i roku – podaj tylko jedną wartość najbardziej reprezentatywną. \"\n",
    "        \"Jeżeli w artykule NIE ma żadnych twardych prognoz liczbowych, ustaw has_forecast=false i zwróć pustą listę forecasts. \"\n",
    "        \"Zawsze odpowiadaj w poprawnym JSON-ie, w którym lista 'forecasts' zawiera tylko unikalne wpisy. \"\n",
    "        \"Cytat – maksymalnie 200 znaków. \"\n",
    "        \"Jeżeli zmienna nie pasuje do listy ('inflation, gdp, interest_rate, unemployment, wages, fx, deficit, public_debt, other'), \"\n",
    "        \"użyj 'other' i w polu 'quote' opisz, co to za zmienna.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "URL artykułu: {url}\n",
    "\n",
    "Tekst artykułu:\n",
    "\\\"\\\"\\\"{article_text}\\\"\\\"\\\"\n",
    "\n",
    "Zwróć odpowiedź jako JSON w następującym schemacie (nie dodawaj żadnych dodatkowych pól):\n",
    "\n",
    "{{\n",
    "  \"has_forecast\": bool,\n",
    "  \"main_topic\": \"krótki opis głównego tematu artykułu\",\n",
    "  \"country\": \"Polska lub inny kraj (jeśli dotyczy)\",\n",
    "  \"forecasts\": [\n",
    "    {{\n",
    "      \"variable\": \"inflation | gdp | interest_rate | unemployment | deficit | public_debt | other\",\n",
    "      \"value\": number lub null jeśli brak jednoznacznej liczby,\n",
    "      \"unit\": \"np. '%' albo 'pp' albo 'mld PLN' albo 'no_number'\",\n",
    "      \"horizon\": \"np. '2023', '2024', '2023 Q4', '2023-2025' itp.\",\n",
    "      \"direction\": \"up | down | stable | unknown\",\n",
    "      \"who_forecasts\": \"podmiot, który formułuje prognozę (np. NBP, KE, mBank, analitycy Bankier.pl itd.)\",\n",
    "      \"quote\": \"krótki cytat z artykułu (max 200 znaków), który zawiera tę prognozę\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Pamiętaj: zwróć TYLKO JSON, bez dodatkowych komentarzy ani tekstu poza nawiasami klamrowymi.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            \n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        data = json.loads(content)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Błąd w wywołaniu OpenAI dla {url}: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# ------------ GŁÓWNA PĘTLA ------------\n",
    "\n",
    "def main():\n",
    "    # sprawdź klucz OpenAI\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"[ERROR] Brak zmiennej środowiskowej OPENAI_API_KEY.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # wczytaj CSV z Bankiera\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Nie udało się wczytać {INPUT_CSV}: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if \"url\" not in df.columns:\n",
    "        print(\"[ERROR] W CSV nie ma kolumny 'url'.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"[INFO] Wczytano {len(df)} rekordów z {INPUT_CSV}\")\n",
    "\n",
    "    # wybierz subset do testów\n",
    "    df_subset = df.head(MAX_ARTICLES).copy()\n",
    "    print(f\"[INFO] Przetwarzam pierwsze {len(df_subset)} artykułów (testowo).\")\n",
    "\n",
    "    # wczytaj checkpoint\n",
    "    last_processed_idx, results = load_checkpoint()\n",
    "    start_idx = last_processed_idx + 1\n",
    "\n",
    "    for idx, row in df_subset.iterrows():\n",
    "        # pomiń już przetworzone\n",
    "        if idx < start_idx:\n",
    "            continue\n",
    "            \n",
    "        url = row[\"url\"]\n",
    "        print(f\"\\n[INFO] ({idx+1}/{len(df_subset)}) Przetwarzam: {url}\")\n",
    "\n",
    "        html = fetch_html(url)\n",
    "        if not html:\n",
    "            print(\"[WARN] Nie udało się pobrać HTML, pomijam.\")\n",
    "            continue\n",
    "\n",
    "        article_text = extract_article_text_bankier(html)\n",
    "        if not article_text or len(article_text) < 200:\n",
    "            print(f\"[WARN] Zbyt mało tekstu ({len(article_text)} znaków), pomijam.\")\n",
    "            continue\n",
    "\n",
    "        llm_data = call_llm_extract(article_text, url)\n",
    "        if llm_data is None:\n",
    "            print(\"[WARN] Błąd w wywołaniu LLM, pomijam.\")\n",
    "            continue\n",
    "\n",
    "        # złącz surowe dane z CSV i wyniki LLM w jeden rekord\n",
    "        result_row = {\n",
    "            \"url\": url,\n",
    "            \"query\": row.get(\"query\", \"\"),\n",
    "            \"title_search\": row.get(\"title_search\", \"\"),\n",
    "            \"snippet_search\": row.get(\"snippet_search\", \"\"),\n",
    "            \"google_detected_date\": row.get(\"google_detected_date\", \"\"),\n",
    "            \"has_forecast\": llm_data.get(\"has_forecast\"),\n",
    "            \"main_topic\": llm_data.get(\"main_topic\"),\n",
    "            \"country\": llm_data.get(\"country\"),\n",
    "            \"forecasts_json\": json.dumps(llm_data.get(\"forecasts\", []), ensure_ascii=False),\n",
    "        }\n",
    "        results.append(result_row)\n",
    "\n",
    "        # checkpoint co CHECKPOINT_INTERVAL artykułów\n",
    "        if len(results) % CHECKPOINT_INTERVAL == 0:\n",
    "            save_checkpoint(results, idx)\n",
    "            \n",
    "        # krótkie opóźnienie\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    if not results:\n",
    "        print(\"[WARN] Brak wyników do zapisania.\")\n",
    "        return\n",
    "\n",
    "    # finalne zapisanie\n",
    "    out_df = pd.DataFrame(results)\n",
    "    out_df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n[INFO] Zapisano {len(out_df)} wierszy do {OUTPUT_CSV}\")\n",
    "    \n",
    "    # usuń checkpoint po zakończeniu\n",
    "    if os.path.exists(CHECKPOINT_CSV):\n",
    "        os.remove(CHECKPOINT_CSV)\n",
    "        print(f\"[INFO] Usunięto checkpoint - przetwarzanie zakończone\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "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\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
